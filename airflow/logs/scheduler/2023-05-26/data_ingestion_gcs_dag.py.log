[2023-05-26T06:51:24.919+0000] {processor.py:157} INFO - Started process (PID=36) to work on /opt/airflow/dags/data_ingestion_gcs_dag.py
[2023-05-26T06:51:24.928+0000] {processor.py:826} INFO - Processing file /opt/airflow/dags/data_ingestion_gcs_dag.py for tasks to queue
[2023-05-26T06:51:24.955+0000] {logging_mixin.py:149} INFO - [2023-05-26T06:51:24.955+0000] {dagbag.py:541} INFO - Filling up the DagBag from /opt/airflow/dags/data_ingestion_gcs_dag.py
[2023-05-26T06:51:25.094+0000] {logging_mixin.py:149} INFO - [2023-05-26T06:51:25.089+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/data_ingestion_gcs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 72, in <module>
    bash_command=f"curl -sS {url} > {path_to_local_home}/{csv_file}"
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 429, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/bash.py", line 150, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 429, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 806, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/task_group.py", line 221, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'download_dataset_task' has already been added to the DAG
[2023-05-26T06:51:25.096+0000] {processor.py:838} WARNING - No viable dags retrieved from /opt/airflow/dags/data_ingestion_gcs_dag.py
[2023-05-26T06:51:25.122+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/data_ingestion_gcs_dag.py took 0.218 seconds
[2023-05-26T06:51:55.509+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/data_ingestion_gcs_dag.py
[2023-05-26T06:51:55.511+0000] {processor.py:826} INFO - Processing file /opt/airflow/dags/data_ingestion_gcs_dag.py for tasks to queue
[2023-05-26T06:51:55.515+0000] {logging_mixin.py:149} INFO - [2023-05-26T06:51:55.515+0000] {dagbag.py:541} INFO - Filling up the DagBag from /opt/airflow/dags/data_ingestion_gcs_dag.py
[2023-05-26T06:51:55.595+0000] {logging_mixin.py:149} INFO - [2023-05-26T06:51:55.590+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/data_ingestion_gcs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 72, in <module>
    bash_command=f"curl -sS {url} > {path_to_local_home}/{csv_file}"
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 429, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/bash.py", line 150, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 429, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 806, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/task_group.py", line 221, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'download_dataset_task' has already been added to the DAG
[2023-05-26T06:51:55.596+0000] {processor.py:838} WARNING - No viable dags retrieved from /opt/airflow/dags/data_ingestion_gcs_dag.py
[2023-05-26T06:51:55.623+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/data_ingestion_gcs_dag.py took 0.118 seconds
